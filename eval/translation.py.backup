# built in
import os
import json
import time

# 3rd party
import torch
import torchvision.transforms as transforms
import PIL
import lpips
from schedule import every, repeat, run_pending
import rasterio
import numpy as np

# custom
from database import Database

# torch setups
GPU = torch.cuda.is_available()
toTensor = transforms.ToTensor()

# lpips setup
lpips_fn = lpips.LPIPS(net='alex')

# logging
import logging
logging.basicConfig(
    filename="eval.log", 
    format=f"[%(asctime)s] [%(levelname)s] [{__file__}] %(message)s", 
    level=logging.INFO, 
    datefmt="%Y-%m-%dT%H:%M:%S%z"
)

def norm_image(z):
    immax = np.max(z)
    immin = np.min(z)
    divisor = immax - immin
    if not divisor:
        divisor = 1
    norm_z = (z-immin)/(divisor)
    norm_z = norm_z.astype(np.float64)
    return norm_z


def rasterio_to_tensor(img):
    divisor = np.amax(img) - np.amin(img)
    if not divisor:
        divisor = 1
    normalized_input = (img - np.amin(img)) / (divisor)
    normalized_input = 2*normalized_input - 1
    stack = np.dstack((normalized_input, normalized_input, normalized_input))
    stack = stack.reshape([1,3,stack.shape[0],stack.shape[0]])
    tensor = torch.Tensor(stack)
    return tensor


def path_to_tensor(path):
    img = rasterio.open(path).read()[0]
    img = norm_image(img)
    tensor = rasterio_to_tensor(img)
    if GPU:
        tensor = tensor.cuda()
    return tensor

def eval_single_file(f, truth_list):
    output = {
        "min": 1
    }
    values = []
    submitted_tensor = path_to_tensor(f)
    basename = os.path.basename(f)
    for truth_f in truth_list:
        truth_tensor = path_to_tensor(os.path.join("/app/truth/translation", truth_f))
        lpips_response = lpips_fn(submitted_tensor, truth_tensor)
        score = lpips_response.item()
        output[f"{f.replace('.png', '')} -> {truth_f.replace('.png', '')}"] = score
        values.append(score)
    output["min"] = min(values)
    return output


def eval_single_folder(submission, truth):
    output = {
        "min": 1
    }
    values = []
    input_files = [f for f in os.listdir(submission) if '.png' in f]
    truth_files = [f for f in os.listdir(truth) if '.png' in f]
    for submitted in input_files:
        submitted_path = os.path.join(submission, submitted)
        submitted_tensor = path_to_tensor(submitted_path)
        for truth_f in truth_files:
            truth_path = os.path.join(truth, truth_f)
            truth_tensor = path_to_tensor(truth_path)
            lpips_response = lpips_fn(submitted_tensor, truth_tensor)
            score = lpips_response.item()
            print(score)
            output[f"{submitted.replace('.png', '')} -> {truth_f.replace('.png', '')}"] = score
            values.append(score)
    output["min"] = min(values)
    return output


def eval_submission(submission, truth="/app/truth/translation"):
    output = {
        "average": 1
    }
    total = []
    to_evaluate = []
    for item in os.listdir(truth):
        full_path = os.path.join(truth, item)
        if os.path.isdir(full_path):
            to_evaluate.append(full_path)
    for folder in to_evaluate:
        input_image = os.path.basename(folder)
        submission_folder = os.path.join(submission, "images", input_image)
        scores = eval_single_folder(submission_folder, folder)
        output[input_image] = scores
        total.append(scores["min"])
        logging.info(f"{input_image} {scores['min']}")
    if total:
        output["average"] = sum(total) / len(total)
    return output


def eval_submission_json(submission, truth="/app/truth/translation"):
    output = {
        "average": 1
    }
    total = []
    with open(os.path.join(truth, 'files.json')) as incoming:
        items = json.load(incoming)
    for submission_f in items:
        truth_files = items[submission_f]
        submission_path = os.path.join(submission, "images", submission_f)
        scores = eval_single_file(submission_path, truth_files)
        output[submission_f] = scores
        total.append(scores["min"])
        logging.info(f"{submission_path} {scores['min']}")
    if total:
        output["average"] = sum(total) / len(total)
    return output


def eval_team(folder):
    team = os.path.basename(folder)
    logging.info(f"Eval Team: {team}")
    scores = []
    submission = os.listdir(folder)
    for submission in submission:
        meta_path = os.path.join(folder, submission, "metadata.json")
        with open(meta_path) as incoming:
            metadata = json.load(incoming)
        if metadata["evaluated"]:
            continue
        logging.info(f"Eval {submission}")
        submission_path = os.path.join(folder, submission)
        score = eval_submission_json(submission_path)
        metadata['scores'] = score
        metadata['evaluated'] = True
        with open(meta_path, 'w') as output:
            output.write(json.dumps(metadata, indent=2))
        logging.info(f"Average: {score['average']}")
        scores.append(score['average'])
    this_best = min(scores, default=1)
    with Database("db/db.sqlite3") as db:
        previous_best = db.get_translation_score_by_team(team)
        if this_best < previous_best:
            db.query(f"UPDATE ImageToImageScores SET score = {this_best} WHERE team = '{team}'")
        

@repeat(every(60).seconds)
def main(path="/app/submissions/valid/translation"):
    os.makedirs(path, exist_ok=True)
    logging.info("Starting scan")
    items = [os.path.join(path, item) for item in os.listdir(path)]
    teams = [f for f in items if os.path.isdir(f)]
    for team in teams:
        eval_team(team)
    logging.info("Done with scan")


if __name__ == "__main__":
    while True:
        run_pending()
        time.sleep(1)
